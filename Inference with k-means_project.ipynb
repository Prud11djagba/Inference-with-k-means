{"cells":[{"cell_type":"code","execution_count":null,"id":"persistent-radius","metadata":{"id":"persistent-radius"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import StandardScaler\n","from numpy.random import uniform\n","from sklearn.datasets import make_blobs\n","from sklearn.datasets import make_moons as moon\n","import seaborn as sns\n","import random\n","import json"]},{"cell_type":"markdown","id":"numerical-trouble","metadata":{"id":"numerical-trouble"},"source":["## Data generating functions"]},{"cell_type":"code","execution_count":null,"id":"right-cherry","metadata":{"id":"right-cherry"},"outputs":[],"source":["# Generating data with one true cluster\n","def generate_data_uniform(n):\n","    datum = np.random.uniform(low=-1, high=1, size=[n, d-1])\n","    last_column = datum.sum(axis = 1)\n","    data = np.column_stack((datum, last_column))\n","    return data\n","def generate_data_uniform_sq(n):\n","    datum = np.random.uniform(low=-1, high=1, size=[n, d-1])\n","    datum_sq = datum**2\n","    last_column = datum_sq.sum(axis = 1)\n","    data = np.column_stack((datum, last_column))\n","    return data\n","def generate_data_uniform_cube(n):\n","    datum = np.random.uniform(low=-1, high=1, size=[n, d-1])\n","    datum_cub = datum**3\n","    last_column = datum_cub.sum(axis = 1)\n","    data = np.column_stack((datum, last_column))\n","    return data\n","def generate_data_uniform_prod(n):\n","    datum = np.random.uniform(low=-1, high=1, size=[n, d-1])\n","#     datum_cub = datum**3\n","    last_column = datum.prod(axis = 1)\n","    data = np.column_stack((datum, last_column))\n","    return data\n","\n","\n","def generate_data_normal(n):\n","    datum = np.random.normal(loc=0.0, scale=1.0, size=[n, d-1])\n","    last_column = datum.sum(axis = 1)\n","    data = np.column_stack((datum, last_column))\n","    return data\n","def generate_data_normal_sq(n):\n","    datum = np.random.normal(loc=0.0, scale=1.0, size=[n, d-1])\n","    datum_sq = datum**2\n","    last_column = datum_sq.sum(axis = 1)\n","    data = np.column_stack((datum, last_column))\n","    return data\n","def generate_data_normal_cube(n):\n","    datum = np.random.normal(loc=0.0, scale=1.0, size=[n, d-1])\n","    datum_sq = datum**3\n","    last_column = datum_sq.sum(axis = 1)\n","    data = np.column_stack((datum, last_column))\n","    return data\n","def generate_data_normal_prod(n):\n","    datum = np.random.normal(loc=0.0, scale=1.0, size=[n, d-1])\n","    last_column = datum.prod(axis = 1)\n","    data = np.column_stack((datum, last_column))\n","    return data\n","\n","\n","def generate_data_gama(n):\n","    datum = np.random.gamma(1,1, size=(n,d-1))\n","    last_column = datum.sum(axis = 1)\n","    data = np.column_stack((datum, last_column))\n","    return data\n","def generate_data_gama_sq(n):\n","    datum = np.random.gamma(1,1, size=(n,d-1))\n","    datum_sq = datum**2\n","    last_column = datum_sq.sum(axis = 1)\n","    data = np.column_stack((datum, last_column))\n","    return data\n","def generate_data_gama_cube(n):\n","    datum = np.random.gamma(1,1, size=(n,d-1))\n","    datum_sq = datum**3\n","    last_column = datum_sq.sum(axis = 1)\n","    data = np.column_stack((datum, last_column))\n","    return data\n","def generate_data_gama_prod(n):\n","    datum = np.random.gamma(1,1, size=(n,d-1))\n","    last_column = datum.prod(axis = 1)\n","    data = np.column_stack((datum, last_column))\n","    return data\n","# def generate_data_example(n):\n","#     data = []\n","#     for _ in range(n):\n","#         xs = []\n","#         for dimension in range(d - 1):\n","#             x = np.random.uniform(low=-1, high=1)\n","#             xs.append(x)\n","#         y = sum(xs)\n","#         datum = xs + [y]\n","#         data.append(datum)\n","#     return data\n","\n","\n","\n","# Generating data with two true clusters\n","\n","def generate_data_unif_2clusters(n):\n","    datum = []\n","    for dimension in range(d-1):\n","        mu = 0\n","        n1 = int(round(n/2,1))\n","        column = np.random.uniform(low=mu, high=mu+5, size = n1)\n","        # x1.append(column)\n","        n2 = int(n-round(n/2,1))\n","        increment = 6\n","        column2 = np.random.uniform(low=mu+increment, high=mu+2*increment, size = n2)\n","        col = np.concatenate((column,column2))\n","        datum.append(col)\n","    datum = np.transpose(datum)\n","    datum_sq = datum**2\n","    last_column = datum_sq.sum(axis= 1)\n","    data = np.column_stack((datum, last_column))\n","    return data\n","\n","def generate_data_normal_2clusters(n):\n","    datum = []\n","    for dimension in range(d-1):\n","        mu = 0\n","        n1 = int(round(n/2,1))\n","        column = np.random.normal(loc=mu, scale=1.0, size = n1)\n","        # x1.append(column)\n","        n2 = int(n-round(n/2,1))\n","        increment = 6\n","        column2 = np.random.normal(loc=mu+increment, scale=1.0, size = n2)\n","        col = np.concatenate((column,column2))\n","        datum.append(col)\n","    datum = np.transpose(datum)\n","    datum_sq = datum**2\n","    last_column = datum_sq.sum(axis= 1)\n","    data = np.column_stack((datum, last_column))\n","    return data\n","\n","def generate_data_conic(n):\n","    n1 = round(n/2)+1\n","    datum = -np.random.gamma(1,1,(n1,d))\n","    reversed_ind = list(reversed(np.arange(d)))\n","    datum1 = -datum[:, reversed_ind]\n","    data = np.concatenate((datum, datum1), axis =0 )[:n]\n","    return data\n","\n","def generate_data_rectangle(n):\n","    n1 = round(n/2)+1\n","    datum = -np.random.gamma(1,1,(n1,d))\n","    reversed_ind = list(reversed(np.arange(d)))\n","    datum1 = -5-datum[:, reversed_ind]\n","    data = np.concatenate((datum, datum1), axis =0 )[:n]\n","    return data\n","\n","\n","def generate_data_unif_normal_clusters(n):\n","    data = []\n","    for dimension in range(d):\n","        mu = 0\n","        n1 = int(round(n/2,1))\n","        column = np.random.uniform(low=mu, high=mu+5, size = n1)\n","        # x1.append(column)\n","        n2 = int(n-round(n/2,1))\n","        increment = 6\n","        column2 = np.random.normal(loc=mu+increment, scale = 1.0, size = n2)\n","        col = np.concatenate((column,column2))\n","        data.append(col)\n","    data = np.transpose(data)\n","    return data\n","\n","# Generating data with 3 true clusters\n","\n","def generate_data_unif_3clusters(n):\n","    data = []\n","    for dimension in range(d):\n","        # mu = np.random.randint(4)\n","        mu = 0\n","        n1 = int(round(n/3,1))\n","        increment = 6\n","        column = np.random.uniform(low=mu, high=mu+increment, size = n1)\n","\n","        increment = 6\n","        column2 = np.random.uniform(low=mu+increment, high=mu+2*increment, size = n1)\n","\n","        n2 = int(n-round(n/3,1))+3\n","        increment = 6\n","        column3 = np.random.uniform(low=mu+2*increment, high=mu+3*increment, size = n2)\n","\n","        col = np.concatenate((column, column2, column3))[:n]\n","        data.append(col)\n","    data = np.transpose(data)\n","    return data\n","\n","def generate_data_normal_3clusters(n):\n","    data = []\n","    for dimension in range(d):\n","        # mu = np.random.randint(4)\n","        mu = 0\n","        n1 = int(round(n/3,1))\n","        increment = 6\n","        column = np.random.normal(loc=mu, scale = 1, size = n1)\n","\n","        increment = 6\n","        column2 = np.random.normal(loc=mu+increment, scale = 1, size = n1)\n","\n","        n2 = int(n-round(n/3,1))+3\n","        increment = 6\n","        column3 = np.random.normal(loc=mu+2*increment, scale = 1, size = n2)\n","\n","        col = np.concatenate((column, column2, column3))[:n]\n","        data.append(col)\n","    data = np.transpose(data)\n","    return data\n","\n","def generate_data_rectangle_norm(n):\n","    n1 = round(n/3)+1\n","    datum = -np.random.gamma(1,1, size = (n1,d))\n","    datum1 = np.random.normal(loc = -5, scale = 1 , size = (n1,d))\n","    reversed_ind = list(reversed(np.arange(d)))\n","    datum2 = -10-datum[:, reversed_ind]\n","    data = np.concatenate((datum, datum1, datum2), axis =0 )[:n]\n","    return data\n","\n","def generate_data_conic_norm(n):\n","    n1 = round(n/3)+1\n","    datum = -np.random.gamma(1,1, size =(n1,d))\n","    datum1 = np.random.normal(loc = 2.5, scale = 1 , size = (n1,d))\n","    reversed_ind = list(reversed(np.arange(d)))\n","    datum2 = 6-datum[:, reversed_ind]\n","    data = np.concatenate((datum, datum1, datum2), axis =0 )[:n]\n","    return data\n","\n","# Special data\n","\n","def generate_data_makeblobs(n, d = 2, n_clusters = 3):\n","    '''\n","    Generates a 2d clustered data\n","    '''\n","    data,_ = make_blobs(n_samples=n, centers = n_clusters, n_features = d, random_state=42)\n","    return data\n","def generate_data_moon(n, d = 2):\n","    '''\n","    Generates a 2d moon shaped data\n","    '''\n","    if d == 2:\n","        X, y = moon(n, noise=0.092)\n","        return X\n","    else:\n","        pass\n","\n","\n","# Data generating methods\n","generate_data_methods = {\n","    # Data with one true cluster\n","    'uniform': generate_data_uniform,\n","    'uniform_sq': generate_data_uniform_sq,\n","    'uniform_cube': generate_data_uniform_cube,\n","    'uniform_prod': generate_data_uniform_prod,\n","    'normal': generate_data_normal,\n","    'normal_sq': generate_data_normal_sq,\n","    'normal_cube': generate_data_normal_cube,\n","    'normal_prod': generate_data_normal_prod,\n","    'gamma': generate_data_gama,\n","    'gamma_sq': generate_data_gama_sq,\n","    'gamma_cube': generate_data_gama_cube,\n","    'gamma_prod': generate_data_gama_prod,\n","    # Data with two true clusters\n","    'uniform_2clust': generate_data_unif_2clusters,\n","    'normal_2clust': generate_data_normal_2clusters,\n","    'conic': generate_data_conic,\n","    'rectangle': generate_data_rectangle,\n","    'unif_normal': generate_data_unif_normal_clusters,\n","    # Data with three true clusters\n","    'unif_3clust': generate_data_unif_3clusters,\n","    'normal_3clust': generate_data_normal_3clusters,\n","    'conic_norm': generate_data_conic_norm,\n","    'rectangle_norm': generate_data_rectangle_norm,\n","    # Special Data\n","    'makeblobs': generate_data_makeblobs,\n","    'moon': generate_data_moon\n","}\n","data_keys = list(generate_data_methods.keys())"]},{"cell_type":"markdown","id":"forward-boost","metadata":{"id":"forward-boost"},"source":["## Online Balanced kmeans implementation"]},{"cell_type":"code","execution_count":null,"id":"rq07ynkgEKG7","metadata":{"id":"rq07ynkgEKG7","colab":{"base_uri":"https://localhost:8080/","height":297},"executionInfo":{"status":"error","timestamp":1720710957382,"user_tz":180,"elapsed":186,"user":{"displayName":"Alfred Kofi ADZIKA","userId":"16750235551968629195"}},"outputId":"491853d8-6714-47cb-a4de-36b7d6186357"},"outputs":[{"output_type":"error","ename":"KeyError","evalue":"'method'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-81e54c52cc65>\u001b[0m in \u001b[0;36m<cell line: 223>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_cluster_size_exp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             ]\n\u001b[0;32m--> 223\u001b[0;31m \u001b[0minf_meths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minference_methods\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'method'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;31m# sns.set_theme(style=\"darkgrid\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-81e54c52cc65>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_cluster_size_exp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             ]\n\u001b[0;32m--> 223\u001b[0;31m \u001b[0minf_meths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minference_methods\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'method'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;31m# sns.set_theme(style=\"darkgrid\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'method'"]}],"source":["class OnlineBalancedKmeans:\n","    def __init__(self, k, d, alpha, beta, inference_method='merge_2', data_distribution = 'uniform'):\n","        self.k = k\n","        self.d = d\n","        # min_value\n","        # max_value\n","\n","        # start editting: 05 / 07/ 2023\n","        # Initialising clusters based on the data type\n","        self.cluster_means = generate_data_methods[data_distribution](self.k)\n","#         if generate_data == 'uniform':\n","#             # self.cluster_means = np.random.uniform(low=-1, high=1, size=[k,d])\n","#             self.cluster_means = np.random.choice()\n","#         elif generate_data = 'normal':\n","#             self.cluster_means = np.random.normal(loc = 0, scale = 1, size = [k,d])\n","#         elif generate_data == 'gamma':\n","#             self.cluster_means = np.random.gamma(1,1, size = [k,d])\n","        # self.cluster_means = np.random.uniform(low=min_value, high=max_value, size=[k,d])\n","        # 05 / 07/ 2023 :end editting\n","\n","        self.cluster_counts = np.ones(k)\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.cluster_indices = []\n","\n","        self.infer = None\n","        method = inference_method['method']\n","        del inference_method['method']\n","        if method == 'euclidean_distance':\n","            self.infer = lambda point: self.infer_euclid_dist(point, **inference_method)\n","        elif method == 'norm_weights':\n","            self.infer = lambda point: self.infer_norm_weights(point, **inference_method)\n","        elif method == 'cluster_size':\n","            self.infer = lambda point: self.infer_cluster_size(point, **inference_method)\n","        elif method == 'weights':\n","            self.infer = lambda point: self.infer_weights(point, **inference_method)\n","        elif method == 'merge':\n","            self.infer = lambda point: self.infer_merge_norm_cl_size(point, **inference_method)\n","        elif method == 'merge_2':\n","            self.infer = lambda point: self.infer_merge_norm_ecl_dis(point, **inference_method)\n","        elif method == 'cls_sz_exp':\n","            self.infer = lambda point: self.infer_cluster_size_exp(point, **inference_method)\n","        #assert self.infer is not None\n","\n","\n","\n","    def compute_weights(self):\n","        '''\n","        return weights that penalize clusters with\n","        many points during assignments\n","        '''\n","        return np.array([self.beta*(np.mean(self.cluster_counts) - cluster_count) / (np.std(self.cluster_counts) + 1e-9) for cluster_count in self.cluster_counts])\n","\n","    def assign_and_update(self, point):\n","        \"\"\"\n","        Does an online k-means update on a single data point.\n","        Args:\n","            point - a 1 x d array\n","        Returns:\n","            An integer in [0, k-1] indicating the assigned cluster.\n","        Updates cluster_means and cluster_counts in place.\n","        For initialization, random cluster means are needed.\n","        \"\"\"\n","\n","        cluster_distances = np.sum(np.sqrt((point - self.cluster_means)**2), axis = 1)\n","        #cluster_distances = np.zeros(self.k)\n","        # for cluster in range(self.k):\n","        #     cluster_distances[cluster] = sum(np.sqrt((point - self.cluster_means[cluster])**2))\n","        #     # cluster_distances[cluster] = sum(np.sqrt((point - self.cluster_means[cluster])**2))\n","        cluster_weights = self.compute_weights()\n","        cluster_index = np.argmin(cluster_distances - cluster_weights)\n","        self.cluster_counts[cluster_index] += 1\n","        self.cluster_means[cluster_index] = self.alpha * np.array(point) + (1 - self.alpha) * self.cluster_means[cluster_index]\n","        self.cluster_indices.append(cluster_index)\n","        #print(cluster_index, cluster_distances[cluster_index], cluster_weights[cluster_index], cluster_distances[0], cluster_weights[0])\n","        return cluster_index\n","\n","    def final_clusters(self, data):\n","        centroids = []\n","        labels = []\n","        cluster_weights = self.compute_weights()\n","        for x in data:\n","            dists = cluster_distances = np.sum(np.sqrt((x - self.cluster_means)**2), axis = 1)\n","            label = np.argmin(dists-cluster_weights)\n","            #centroids.append(self.centroids[label])\n","            labels.append(label)\n","        # cluster_counts = np.unique(labels, return_counts= True)\n","        return labels\n","\n","    def view_final_clusters(self, data):# for 2d\n","        for index, data_point in enumerate(data):\n","            self.assign_and_update(data_point)\n","        labels = self.final_clusters(data)\n","        class_indices = self.cluster_indices\n","        #print(len(class_indices),len(true_labels[:index]))\n","        sns.scatterplot(x=[X[0] for X in data],\n","                      y=[X[1] for X in data],\n","                      hue=labels,\n","                      #style=true_labels,\n","                      palette=\"deep\",\n","                      legend=None\n","                      )\n","        class_centers = self.cluster_means\n","        class_counts = np.unique(labels, return_counts=True)[1]\n","        # class_counts = kmeans.cluster_counts\n","        X = [x for x, _ in class_centers]\n","        Y = [y for _, y in class_centers]\n","        plt.plot(X, Y, 'r+', markersize=10, )\n","        # add labels to all points\n","        # np.unique(labels, return_counts=True)\n","        for (xi, yi, counts) in zip(X, Y, class_counts):\n","            plt.text(xi, yi, counts, va='bottom', ha='center')\n","        plt.title('Final iteration')\n","        plt.show()\n","\n","    def infer_old(self, point):\n","        # Warning: Only works for d=2. TODO: Generalize to arbitrary dimensions.\n","        # Find closest mean in x coordinate.\n","        distances = np.zeros(self.k)\n","        for cluster in range(self.k):\n","            distances[cluster] = (point[0] - self.cluster_means[cluster, 0])**2\n","        cluster_index = np.argmin(distances)\n","        # Return y coordinate of closest mean.\n","        return self.cluster_means[cluster_index, 1]\n","\n","    def infer_euclid_dist(self, point):\n","        \"\"\"\n","        point = (d,) array ie (1 by d)\n","        \"\"\"\n","\n","        # Find the closest mean in the known coordinates\n","        distances = np.sqrt( np.sum( ( point[ : self.d-2 ] - self.cluster_means[ :, : self.d-2] )**2, axis = 1 ) )\n","        cluster_index = np.argmin( distances )\n","        inferred_val = self.cluster_means[cluster_index, self.d-1]\n","        # Return the unknown coordinates of the closest mean\n","        return inferred_val\n","\n","    def infer_norm_weights(self, point, beta = 7):\n","        distances = np.sqrt( np.sum( ( point[ : self.d-2 ] - self.cluster_means[ :, : self.d-2] )**2, axis = 1 ) )\n","        weights = np.exp(-beta*distances)\n","        normalized_weights = weights / sum(weights)\n","        # Return the estimate of the d_th coordinates of the closest mean using the weights\n","        inferred_val = sum(normalized_weights * self.cluster_means[:,d-1])\n","        return inferred_val\n","\n","    def infer_cluster_size(self, point):\n","        point = np.array(point)\n","        # Find the 5 closest cluster means to the point and the total number of points in those clusters\n","        distances =  np.sum( ( point[ : self.d-2 ] - self.cluster_means[ :, : self.d-1] )**2, axis = 1 )\n","        cluster_indices = []\n","        for cluster in range(5):\n","            index = np.argmin( distances )\n","            cluster_indices.append(index)\n","\n","            distances = np.delete(distances, index)\n","            #print(\"index:{} \\n dist:{} \\n means:{} \\n count:{}\".format(index, distances[:3], self.cluster_means[:3], self.cluster_counts[index]))\n","\n","        total_counts = np.sum(self.cluster_counts[cluster_indices])\n","        #print(total_counts)\n","        # Compute their means\n","        inferred_vals = self.cluster_counts[cluster_indices]/total_counts @ self.cluster_means[cluster_indices,:]\n","        inferred_val = inferred_vals[self.d-1]\n","        return inferred_val\n","\n","    def infer_weights(self, point, alpha=0.4, beta=7):\n","        overall_mean = self.cluster_counts @ self.cluster_means[:,d-1]/sum(self.cluster_counts)\n","        distances = np.sqrt( np.sum( ( point[ : self.d-2 ] - self.cluster_means[ :, : self.d-2] )**2, axis = 1 ) )\n","        weights = np.exp(-beta*distances)\n","        normalized_weights = weights / sum(weights)\n","        inferred_val = alpha*overall_mean + (1.0-alpha)*sum(normalized_weights * self.cluster_means[:,d-1])\n","        return inferred_val\n","\n","    def infer_merge_norm_cl_size(self, point, alpha=0.8, beta = 7):\n","        infered_val = alpha * self.infer_norm_weights(point, beta) + (1 - alpha) * self.infer_cluster_size(point)\n","        return infered_val\n","\n","\n","    def infer_merge_norm_ecl_dis(self, point, alpha=0.9, beta = 7):\n","        infered_val = alpha * self.infer_norm_weights(point, beta) + (1 - alpha) * self.infer_euclid_dist(point)\n","        return infered_val\n","    def infer_cluster_size_exp(self, point, beta=8):\n","        # Find the 5 closest cluster means to the point and the total number of points in those clusters\n","        distances =  np.sum( ( point[ : self.d-2 ] - self.cluster_means[ :, : self.d-1] )**2, axis = 1 )\n","        cluster_indices = []\n","        for cluster in range(5):\n","            index = np.argmin( distances )\n","            cluster_indices.append(index)\n","\n","            distances = np.delete(distances, index)\n","            #print(\"index:{} \\n dist:{} \\n means:{} \\n count:{}\".format(index, distances[:3], self.cluster_means[:3], self.cluster_counts[index]))\n","\n","        total_counts = np.sum(self.cluster_counts[cluster_indices])\n","        #print(total_counts)\n","        # Compute their means\n","        inferred_vals = np.exp(-beta * self.cluster_counts[cluster_indices]/total_counts ) @ self.cluster_means[cluster_indices,:]\n","        inferred_val = inferred_vals[self.d-1]\n","        return inferred_val\n","\n","# Optimised Inference Methods\n","inference_methods = [\n","    {'method': 'euclidean_distance'}, {'method': 'norm_weights', 'beta': 7 }, {'method': 'cluster_size'}\n","]+[\n","    {'method': 'weights', 'alpha': 0.4, 'beta':7 }\n","]+ [\n","    {'method': 'merge', 'alpha': 0.8, 'beta': 7}\n","]+ [\n","    {'method': 'merge_2', 'alpha': 0.9, 'beta': 7}\n","] + [\n","    {'method': 'cls_sz_exp', 'beta': 8}\n","]\n","\n","kmeans = OnlineBalancedKmeans(k = 4, d = 2, alpha = 0.5, beta = 0.5, inference_method=inference_methods[1])\n","\n","inference_functions = [\n","                kmeans.infer_euclid_dist,\n","                kmeans.infer_norm_weights,\n","                kmeans.infer_cluster_size,\n","                kmeans.infer_weights,\n","                kmeans.infer_merge_norm_cl_size,\n","                kmeans.infer_merge_norm_ecl_dis,\n","                kmeans.infer_cluster_size_exp\n","            ]\n","inf_meths = [inference_methods[i]['method'] for i in range(7)]\n","# sns.set_theme(style=\"darkgrid\")"]},{"cell_type":"markdown","id":"sixth-banner","metadata":{"id":"sixth-banner"},"source":["## Plot Functions"]},{"cell_type":"markdown","id":"black-pizza","metadata":{"id":"black-pizza"},"source":["### Ploting the errors for a generated data"]},{"cell_type":"code","execution_count":null,"id":"superior-michael","metadata":{"id":"superior-michael"},"outputs":[],"source":["def plot_errors(\n","    error_evol_data, ## dim = (len(inf_meths) , any natural num.)\n","    data_key = '',\n","    fname='.png',\n","    title = 'Errors for each inference methods',\n","    x = list(range(0,10001, 1000)),\n","    save = False\n","):\n","    fig, (ax1) = plt.subplots( 1,1, figsize = (10, 7) )\n","    thres = 2*10**20\n","    legends = []\n","    for inf_index, inf_meth in enumerate(inf_meths):\n","        if error_evol_data[inf_index,0] <= thres:\n","            legends.append(inf_meth)\n","            ax1.plot(x, error_evol_data[inf_index])\n","            ax1.set_xlabel('Number of datapoint assigned')\n","            ax1.set_ylabel('Inference errors')\n","            ax1.legend(legends)\n","    plt.suptitle('{}{}'.format(title, data_key))\n","\n","    if save == True: plt.savefig('{}{}'.format(data_key, fname))\n","\n","\n","# Ploting the errors of all the data seperately\n","# for data, data_key in zip(data_iter, data_dist_keys):\n","#     plot_errors(data = data.T, title='Errors for each inference methods:', data_key = data_key)"]},{"cell_type":"markdown","id":"isolated-plumbing","metadata":{"id":"isolated-plumbing"},"source":["### Plots of clusters after every 'n_assigned' data points"]},{"cell_type":"code","execution_count":null,"id":"u7A8dpCSjcJB","metadata":{"id":"u7A8dpCSjcJB"},"outputs":[],"source":["\n","def plot_obk(\n","    data_key, k=20, d=2, alpha=0.3, beta=0.07, n_train=100,\n","    inference_method={'method': 'weights', 'alpha': 0.0},\n","    n_assigned = 200,\n","    save = False\n","):\n","    X_train = generate_data_methods[data_key](n_train)\n","    kmeans = OnlineBalancedKmeans(k, d, alpha, beta, inference_method=inference_method)\n","    for index, data_point in enumerate(X_train):\n","        kmeans.assign_and_update(data_point)\n","        labels = kmeans.cluster_indices\n","        if index % n_assigned == 0 or index+1 == len(X_train ):\n","            class_centers = kmeans.cluster_means\n","            class_labels = kmeans.cluster_indices\n","            sns.scatterplot(x=[X[0] for X in X_train[:index,:]],\n","                            y=[X[1] for X in X_train[:index,:]],\n","                            hue=class_labels[:index],\n","                            # style=true_labels[:index],\n","                            palette=\"deep\",\n","                            legend=None\n","                            )\n","            plt.plot([x for x, _ in class_centers],\n","                    [y for _, y in class_centers],\n","                    'r+',\n","                    markersize=10,\n","                    )\n","            plt.title('iteration {}'.format(index))\n","            plt.show()\n","\n","    sns.scatterplot(x=[X[0] for X in X_train],\n","                    y=[X[1] for X in X_train],\n","                    hue=labels,\n","                    #style=true_labels,\n","                    palette=\"deep\",\n","                    legend=None\n","                    )\n","    class_centers = kmeans.cluster_means\n","    class_counts = np.unique(labels, return_counts=True)[1]\n","    X = [x for x, _ in class_centers]\n","    Y = [y for _, y in class_centers]\n","    plt.plot(X, Y, 'r+', markersize=10, )\n","    for (xi, yi, counts) in zip(X, Y, class_counts):\n","        plt.text(xi, yi, counts, va='bottom', ha='center')\n","    plt.title('Final iteration of {} data'.format(data_key))\n","    if save == True: plt.savefig('Final iteration_{}.png'.format(data_key))\n","    plt.show()\n","\n","# plot_obk(data_key='uniform')"]},{"cell_type":"markdown","id":"numeric-staff","metadata":{"id":"numeric-staff"},"source":["### Ploting final iteration"]},{"cell_type":"code","execution_count":null,"id":"3dU9S-4d06X8","metadata":{"id":"3dU9S-4d06X8"},"outputs":[],"source":["def plot_final_assignment(\n","    data_key,\n","    inference_method={'method': 'weights', 'alpha': 0.0},\n","    k = 20,\n","    d = 2,\n","    alpha = 0.3,\n","    beta = 0.01,\n","    n_train = 100\n","):\n","    # Create k-means clustering object\n","    kmeans = OnlineBalancedKmeans(k, d, alpha, beta, inference_method=inference_method.copy(), data_distribution=data_key)\n","    # Create a dataset of 2D distributions\n","    X_train = generate_data_methods[data_key](n_train)\n","\n","    for index, data_point in enumerate(X_train):\n","        kmeans.assign_and_update(data_point)\n","    labels = kmeans.final_clusters(X_train)\n","    # class_indices = kmeans.cluster_indices\n","    sns.scatterplot(x=[X[0] for X in X_train],\n","                    y=[X[1] for X in X_train],\n","                    hue=labels,\n","                    #style=true_labels,\n","                    palette=\"deep\",\n","                    legend=None\n","                    )\n","    class_centers = kmeans.cluster_means\n","    class_counts = np.unique(labels, return_counts=True)[1]\n","    X = [x for x, _ in class_centers]\n","    Y = [y for _, y in class_centers]\n","    plt.plot(X, Y, 'r+', markersize=10, )\n","    for (xi, yi, counts) in zip(X, Y, class_counts):\n","        plt.text(xi, yi, counts, va='bottom', ha='center')\n","    plt.title('Final iteration after clustering')\n","#     plt.xlabel('N(0,1)')\n","#     plt.ylabel('N(0,1)')\n","    plt.savefig('normal_final_ite.png')\n","    plt.show()\n","\n","# Examples\n","#plot_final_assignment(data_key = 'gamma')"]},{"cell_type":"markdown","id":"dirty-bhutan","metadata":{"id":"dirty-bhutan"},"source":["### A 3d view of the online balanced k-means"]},{"cell_type":"code","execution_count":null,"id":"ahead-escape","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":315},"id":"ahead-escape","executionInfo":{"status":"error","timestamp":1720710472910,"user_tz":180,"elapsed":179,"user":{"displayName":"Alfred Kofi ADZIKA","userId":"16750235551968629195"}},"outputId":"66a96db5-405a-4699-db1c-dcf9a06a84b9"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'data_dist_keys' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-aaee5c800504>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mplot_obk_3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-8-aaee5c800504>\u001b[0m in \u001b[0;36mplot_obk_3d\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_obk_3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata_dist_key\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_dist_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_data_methods\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_dist_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOnlineBalancedKmeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'method'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'weights'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'alpha'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'beta'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m7\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_distribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_dist_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'data_dist_keys' is not defined"]}],"source":["# Visualise the obk in 3d\n","def plot_obk_3d():\n","    d = 3\n","    for data_dist_key in data_dist_keys:\n","        X_train = generate_data_methods[data_dist_key](1000)\n","        kmeans = OnlineBalancedKmeans(k, d, alpha = 0.2, beta= 0.01, inference_method={'method': 'weights', 'alpha': 0.4, 'beta':7 }, data_distribution = data_dist_key)\n","        for index, data_point in enumerate(X_train):\n","            kmeans.assign_and_update(data_point)\n","            # if index == len(X_train )- 1:\n","        labels = kmeans.final_clusters(X_train)\n","        sns.set_style (\"darkgrid\")\n","        plot_mean = 3\n","        min_num = 30\n","        x=[X[0] for X in X_train[:index+1,:]]\n","        y=[X[1] for X in X_train[:index+1,:]]\n","        z=[X[2] for X in X_train[:index+1,:]]\n","    #         plot1 =  np.random.normal (plot_mean, 1, size = min_num)\n","    #         plot2 = np.random.normal (plot_mean, 1, size = min_num)\n","    #         plot3 = np.random.normal (plot_mean, 1, size = min_num)\n","        plt.figure (figsize = (5, 4))\n","        seaborn_plot = plt.axes (projection='3d')\n","        print (type (seaborn_plot))\n","        seaborn_plot.scatter3D (x, y, z, c= labels[:index+1], alpha = 1)\n","        # seaborn_plot.scatter3D (plot1, plot2, plot3)\n","        seaborn_plot.set_xlabel ('x')\n","        seaborn_plot.set_ylabel ('y')\n","        seaborn_plot.set_zlabel ('z')\n","        plt.title('{}'.format(data_dist_key))\n","        #plt.savefig(\"unif_3d\")\n","        plt.show ()\n","\n","\n","plot_obk_3d()"]},{"cell_type":"markdown","id":"surgical-thousand","metadata":{"id":"surgical-thousand"},"source":["## Calculating Errors and Losses"]},{"cell_type":"markdown","id":"8O_gxnYgyQeX","metadata":{"id":"8O_gxnYgyQeX"},"source":["#### Error Computing and the loss function"]},{"cell_type":"code","execution_count":null,"id":"hindu-strip","metadata":{"id":"hindu-strip"},"outputs":[],"source":["# Evaluation script.\n","def compute_error(true_value, predicted_value):\n","    return np.sum((true_value - predicted_value) ** 2)\n","\n","def kmeans_loss(kmeans, training_data):\n","    # Initialize loss to zero.\n","    loss = 0\n","    # For every point in training data set\n","    for point in training_data:\n","    #    Find coordinates of closest cluster center.\n","    #    Compute squared distance between point and closest cluster center.\n","        distances =  np.sum( ( point[ : kmeans.d-1 ] - kmeans.cluster_means[ :, : kmeans.d-1] )**2, axis = 1 )\n","        #    Add distance to loss.\n","        loss += np.min(distances, axis = 0)\n","    return loss"]},{"cell_type":"markdown","id":"abstract-constitution","metadata":{"id":"abstract-constitution"},"source":["### Computing the errors"]},{"cell_type":"code","execution_count":null,"id":"insured-terrorism","metadata":{"id":"insured-terrorism"},"outputs":[],"source":["def compute_losses_errors(\n","    k = 20,\n","    d = 3,\n","    alpha = 0.2,\n","    beta = 0.01,\n","    n_train = 10000,\n","    n_test = 1000,\n","    n_repeats = 1\n","    ):\n","    #inference_methods = ['higher_dimension', 'norm_weights','cluster_size', 'weights' ]\n","    for inference_method in inference_methods:\n","        errors = []\n","        for generate_data in data_generation_methods:\n","            distribution_errors = []\n","            for repeat in range(n_repeats):\n","                data = generate_data(n_train + n_test)\n","                training_data = data[:n_train]\n","                test_data = data[n_train:]\n","                assert len(training_data) == n_train\n","                assert len(test_data) == n_test\n","                #training_data = generate_data(n_train)\n","                #test_data = generate_data(n_test)\n","                kmeans = OnlineBalancedKmeans(k, d, alpha, beta, inference_method=inference_method.copy()) # TODO. Copy might not be necessary.\n","                for datum in training_data:\n","                    kmeans.assign_and_update(datum)\n","\n","                for datum in test_data:\n","                    true_value = datum[-1]\n","                    predicted_value = kmeans.infer(datum)\n","                    error = compute_error(true_value, predicted_value)\n","                    distribution_errors.append(error)\n","\n","            errors.append(distribution_errors)\n","\n","            print('[{}, {}] \\t Average error: {}'.format(generate_data.__name__, inference_method, np.mean(distribution_errors)))\n","        print('[{}] \\t Average error: {}'.format(inference_method, np.mean(errors)))\n","    return errors"]},{"cell_type":"markdown","id":"conscious-sharp","metadata":{"id":"conscious-sharp"},"source":["### Computing error updated codes"]},{"cell_type":"code","execution_count":null,"id":"legislative-examination","metadata":{"id":"legislative-examination"},"outputs":[],"source":["k = 20 # number of pseudo-clusters\n","d = 2 # number of features\n","alpha_ = 0.2 # Learning rate associated with the means\n","beta_ = 0.01 # Balances the size of the clusters\n","n_train = 1000\n","n_test = 100\n","n_repeats = 10"]},{"cell_type":"code","execution_count":null,"id":"interim-developer","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"interim-developer","executionInfo":{"status":"error","timestamp":1720710493936,"user_tz":180,"elapsed":242,"user":{"displayName":"Alfred Kofi ADZIKA","userId":"16750235551968629195"}},"outputId":"537dfb66-9745-4127-b9bd-1e9e75806e39"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'data_dist_keys' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-f5fe77d88ade>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# distributions_loss_mean = {}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mloss_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata_dist_key\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_dist_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m# distribution_errors = []\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#distribution_loss_repeat = {}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'data_dist_keys' is not defined"]}],"source":["#data = generate_data_methods[data_dist_key](n_train + n_test)\n","inference_errors = {}\n","for inference_method in inference_methods:\n","\n","    distribution_error_mean = {}\n","    # distributions_loss_mean = {}\n","    loss_vals = {}\n","    for data_dist_key in data_dist_keys:\n","        # distribution_errors = []\n","        #distribution_loss_repeat = {}\n","\n","        data = generate_data_methods[data_dist_key](n_train + n_test)\n","        training_data = data[:n_train]\n","        test_data = data[n_train:]\n","        assert len(training_data) == n_train\n","        assert len(test_data) == n_test\n","        #training_data = generate_data(n_train)\n","        #test_data = generate_data(n_test)\n","        kmeans = OnlineBalancedKmeans(k, d, alpha = alpha_, beta = beta_, inference_method=inference_method.copy(), data_distribution=data_dist_key) # TODO. Copy might not be necessary.\n","        # loss_vals = []\n","\n","        for datum in training_data:\n","            kmeans.assign_and_update(datum)\n","        cost = kmeans_loss(kmeans, training_data)\n","        loss_vals[data_dist_key]= round(cost, 4)\n","        #distribution_loss_repeat.append(loss_vals)\n","        #distribution_loss_mean = np.mean(distribution_loss_repeat, axis = 0)\n","        distribution_errors = []\n","        for repeat in range(n_repeats):\n","            for datum in test_data:\n","                true_value = datum[-1]\n","                predicted_value = kmeans.infer(datum)\n","                error = compute_error(true_value, predicted_value)\n","                distribution_errors.append(round(error, 4))\n","        distribution_error_mean[data_dist_key] = np.mean(distribution_errors)\n","\n","        #distributions_error_mean.append(distribution_error_mean)  # dim = n_dist by n_test # Each row correspond to a distributions list of errors\n","        #distributions_loss_mean.append(distribution_loss_mean)  # dim = n_dist by n_train\n","        #print('[{},\\t {}] \\t Average error: {}'.format(data_dist_key, inference_method, round(np.mean(distributions_error_mean), 4)))\n","    # A three dimensional metric of the Inference errors\n","    inference_method_ = json.dumps(inference_method)\n","    inference_errors[inference_method_] = list(distribution_error_mean.values())\n","    print('[{}] \\t Average error: {}'.format(inference_method, round(np.mean(list(distribution_error_mean.values())), 4) ))\n","final_error_data = inference_errors"]},{"cell_type":"markdown","id":"diagnostic-lease","metadata":{"id":"diagnostic-lease"},"source":["### Visualising the various generated data"]},{"cell_type":"code","execution_count":null,"id":"personalized-british","metadata":{"id":"personalized-british"},"outputs":[],"source":["    # Viewing the first 12 plots\n","def graph_gen_data_a():\n","        rows = [0,1,2,3]*3\n","        cols = [0]*4 + [1]*4 + [2]*4\n","        #print( '{}\\n{}'.format(cols,rows) )\n","        rcd = list(zip(rows,cols, data_dist_keys))\n","        # print('n_rows\\tn_columns\\tDistribution')\n","        # for r,c,d in rcd:\n","        #     print('{}\\t{}\\t{}'.format(r,c,d))\n","        k = 20\n","        d = 2\n","        # Create k-means clustering plots\n","\n","        # Set the figure size\n","\n","        fig,ax = plt.subplots(4,3,figsize = (15, 25))\n","        i = 0\n","\n","        for r,c,data_dist_key in rcd:\n","\n","            kmeans = OnlineBalancedKmeans(k, d, alpha=0.2, beta=0.01, inference_method={'method': 'weights', 'alpha': 0.0}, data_distribution=data_dist_key)\n","            # kmeans = OnlineBalancedKmeans(k=k, d=2, alpha=0.2, beta=0.2)\n","            # Create a dataset of 2D distributions\n","            X_train = generate_data_methods[data_dist_key](100)\n","\n","            for index, data_point in enumerate(X_train):\n","                kmeans.assign_and_update(data_point)\n","            labels = kmeans.final_clusters(X_train)\n","            class_indices = kmeans.cluster_indices\n","            sns.scatterplot(x=[X[0] for X in X_train],\n","                            y=[X[1] for X in X_train],\n","                            hue=labels,\n","                            #style=true_labels,\n","                            palette=\"deep\",\n","                            legend=None,\n","                            ax = ax[r,c]\n","                            )\n","\n","            class_centers = kmeans.cluster_means\n","            class_counts = np.unique(labels, return_counts=True)[1]\n","            # class_counts = kmeans.cluster_counts\n","            X = [x for x, _ in class_centers]\n","            Y = [y for _, y in class_centers]\n","            ax[r,c].plot(X, Y, 'r+', markersize=10, )\n","            # add labels to all points\n","            for (xi, yi, counts) in zip(X, Y, class_counts):\n","                ax[r,c].text(xi, yi, counts, va='bottom', ha='center')\n","            ax[r,c].set_title('Final iteration {}'.format(data_dist_key))\n","            # i += 1\n","            plt.savefig('{}_final_iter.png'.format(data_dist_key))\n","            #plt.savefig('{}_final_iter.png'.format(data_dist_key))\n","            #plt.show()\n","        # plt.savefig('Data_vrs_OBKmeans.png')\n","        fig.tight_layout()\n","\n","# graph_gen_data_a()"]},{"cell_type":"markdown","id":"certain-temperature","metadata":{"id":"certain-temperature"},"source":["### Performance of the Hyperparameters"]},{"cell_type":"markdown","id":"sufficient-plaintiff","metadata":{"id":"sufficient-plaintiff"},"source":["#### Hyperparameter k"]},{"cell_type":"code","execution_count":null,"id":"broke-notification","metadata":{"id":"broke-notification"},"outputs":[],"source":["# Generating range of values to loop over\n","kss = np.linspace(0,1000,11).tolist()\n","kss.pop(0)\n","kss.insert(0,10)\n","kss = [int(k) for k in kss]"]},{"cell_type":"code","execution_count":null,"id":"southern-soldier","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"southern-soldier","executionInfo":{"status":"error","timestamp":1720710506705,"user_tz":180,"elapsed":196,"user":{"displayName":"Alfred Kofi ADZIKA","userId":"16750235551968629195"}},"outputId":"ef224370-ce74-4769-e5e2-cf6b0d9b07d4"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'data_dist_keys' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-e2dec76365f1>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mkdict_errors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mkdict_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata_dist_key\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_dist_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;31m# distribution_errors = []\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m#distribution_loss_repeat = {}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'data_dist_keys' is not defined"]}],"source":["# number of pseudo-clusters Type b\n","d = 2 # number of features\n","#alpha_ = 0.2 # Learning rate associated with the means\n","#beta_ = 0.01 # Balances the size of the clusters\n","n_train = 10000\n","n_test = 1000\n","n_repeats = 1\n","alpha = 0.2\n","beta = 0.01\n","n_ind = 1000\n","\n","kdict_errors = {}\n","kdict_loss = {}\n","for k in kss:\n","        a = 1\n","        loss_vals = {}\n","        cost = {}\n","        error = {}\n","        kdict_errors[k] = []\n","        kdict_loss[k] = []\n","        for data_dist_key in data_dist_keys:\n","            # distribution_errors = []\n","            #distribution_loss_repeat = {}\n","\n","            data = generate_data_methods[data_dist_key](n_train + n_test)\n","            training_data = data[:n_train]\n","            test_data = data[n_train:]\n","            assert len(training_data) == n_train\n","            assert len(test_data) == n_test\n","            #training_data = generate_data(n_train)\n","            #test_data = generate_data(n_test)\n","            kmeans = OnlineBalancedKmeans( k, d, alpha = alpha, beta = beta, inference_method={'method': 'weights', 'alpha': 0.4, 'beta':7 } , data_distribution=data_dist_key) # TODO. Copy might not be necessary.\n","            # Computing the losses and error and storing them in a dict\n","            inference_functions = [\n","                kmeans.infer_euclid_dist,\n","                kmeans.infer_norm_weights,\n","                kmeans.infer_cluster_size,\n","                kmeans.infer_weights,\n","                kmeans.infer_merge_norm_cl_size,\n","                kmeans.infer_merge_norm_ecl_dis,\n","                kmeans.infer_cluster_size_exp\n","            ]\n","            # Initializing losses\n","            cost[data_dist_key] = [ kmeans_loss(kmeans, training_data) ]\n","            # Initializing errors\n","            total_error = np.zeros_like(inference_functions)\n","            for datum in test_data:\n","                true_value = datum[-1]\n","                predicted_values = [infer(datum) for infer in inference_functions ] #dim = (n_inf,) #kmeans.infer(datum)\n","                total_error += np.array([compute_error(true_value, pred_value) for pred_value in predicted_values])\n","                # total_error dim = (n_inf)\n","            error[data_dist_key] = [total_error]\n","            for index, datum in enumerate(training_data):\n","                kmeans.assign_and_update(datum)\n","                if index%n_ind == 0:\n","                    # Compute losses after each 1000 assignments\n","                    cost[data_dist_key].append( kmeans_loss(kmeans, training_data) )\n","                    # Compute errors after each 1000 assignments\n","                        # Predicted values for all inference methods\n","                    predicted_values = [infer(datum) for infer in inference_functions ]\n","                    total_error = np.zeros_like(inference_functions)\n","                    for datum in test_data:\n","                        true_value = datum[-1]\n","                        predicted_values = [infer(datum) for infer in inference_functions ] #dim = (n_inf,) #kmeans.infer(datum)\n","                        total_error += np.array([compute_error(true_value, pred_value) for pred_value in predicted_values])\n","                        # total_error dim = (n_inf)\n","                    error[data_dist_key].append( total_error )\n","        kdict_errors[k] = list(error.values())\n","        #mean_losses = np.mean(list(cost.values()), 0) # Values recorded for every 100 iteration\n","        kdict_loss[k] = list(cost.values())\n","        # mean_errors = np.mean(list(error.values()), 0) # Values recorded for every 100 iteration\n","        # kdict_errors[i] = mean_errors"]},{"cell_type":"markdown","id":"palestinian-explanation","metadata":{"id":"palestinian-explanation"},"source":["#### Hyperparameter self.alpha"]},{"cell_type":"code","execution_count":null,"id":"opposed-campbell","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"opposed-campbell","executionInfo":{"status":"error","timestamp":1720710511519,"user_tz":180,"elapsed":223,"user":{"displayName":"Alfred Kofi ADZIKA","userId":"16750235551968629195"}},"outputId":"df715cd2-2d11-467a-8d1f-9a72f88135d1"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'data_dist_keys' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-b57d68045a3d>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0malpha_dict_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0malpha_dict_errors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata_dist_key\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_dist_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m# distribution_errors = []\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m#distribution_loss_repeat = {}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'data_dist_keys' is not defined"]}],"source":["    # alpha vrs losses vrs errors\n","d = 2 # number of features\n","#alpha_ = 0.2 # Learning rate associated with the means\n","#beta_ = 0.01 # Balances the size of the clusters\n","k = 50\n","n_train = 10000\n","n_test = 1000\n","n_repeats = 1\n","#alpha = 0.2\n","beta = 0.01\n","alphas = np.linspace(0,1, 11 )\n","alpha_dict_loss = {}\n","alpha_dict_errors = {}\n","n_ind = 1000\n","\n","for alpha in alphas:\n","        a = 1\n","        loss_vals = {}\n","        cost = {}\n","        error = {}\n","        alpha_dict_loss[alpha] = []\n","        alpha_dict_errors[alpha] = []\n","        for data_dist_key in data_dist_keys:\n","            # distribution_errors = []\n","            #distribution_loss_repeat = {}\n","\n","            data = generate_data_methods[data_dist_key](n_train + n_test)\n","            training_data = data[:n_train]\n","            test_data = data[n_train:]\n","            assert len(training_data) == n_train\n","            assert len(test_data) == n_test\n","            #training_data = generate_data(n_train)\n","            #test_data = generate_data(n_test)\n","            kmeans = OnlineBalancedKmeans( k, d, alpha = alpha, beta = beta, inference_method={'method': 'weights', 'alpha': 0.4, 'beta':7 } , data_distribution=data_dist_key) # TODO. Copy might not be necessary.\n","            # Computing the losses and error and storing them in a dict\n","            inference_functions = [\n","                kmeans.infer_euclid_dist,\n","                kmeans.infer_norm_weights,\n","                kmeans.infer_cluster_size,\n","                kmeans.infer_weights,\n","                kmeans.infer_merge_norm_cl_size,\n","                kmeans.infer_merge_norm_ecl_dis,\n","                kmeans.infer_cluster_size_exp\n","            ]\n","            # Initializing losses\n","            cost[data_dist_key] = [ kmeans_loss(kmeans, training_data) ]\n","            # Initializing errors\n","            total_error = np.zeros_like(inference_functions)\n","            for datum in test_data:\n","                true_value = datum[-1]\n","                predicted_values = [infer(datum) for infer in inference_functions ] #dim = (n_inf,) #kmeans.infer(datum)\n","                total_error += np.array([compute_error(true_value, pred_value) for pred_value in predicted_values])\n","                # total_error dim = (n_inf)\n","            error[data_dist_key] = [total_error]\n","            for index, datum in enumerate(training_data):\n","                kmeans.assign_and_update(datum)\n","                if index%n_ind == 0:\n","                    # Compute losses after each 1000 assignments\n","                    cost[data_dist_key].append( kmeans_loss(kmeans, training_data) )\n","                    # Compute errors after each 1000 assignments\n","                        # Predicted values for all inference methods\n","                    predicted_values = [infer(datum) for infer in inference_functions ]\n","                    total_error = np.zeros_like(inference_functions)\n","                    for datum in test_data:\n","                        true_value = datum[-1]\n","                        predicted_values = [infer(datum) for infer in inference_functions ] #dim = (n_inf,) #kmeans.infer(datum)\n","                        total_error += np.array([compute_error(true_value, pred_value) for pred_value in predicted_values])\n","                        # total_error dim = (n_inf)\n","                    error[data_dist_key].append( total_error )\n","        alpha_dict_errors[alpha].append(list(error.values()))\n","        #mean_losses = np.mean(list(cost.values()), 0) # Values recorded for every 100 iteration\n","        alpha_dict_loss[alpha].append(list(cost.values()))\n","\n","#         mean_losses = np.mean(list(cost.values()), 0) # Values recorded for every 100 iteration\n","#         alpha_dict_loss[alpha].append(mean_losses)\n","#         mean_errors = np.mean(list(error.values()), 0) # Values recorded for every 100 iteration\n","#         alpha_dict_errors[alpha].append(mean_errors)"]},{"cell_type":"markdown","id":"sporting-activity","metadata":{"id":"sporting-activity"},"source":["#### Hyperparameter self.beta"]},{"cell_type":"code","execution_count":null,"id":"determined-lloyd","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"determined-lloyd","executionInfo":{"status":"error","timestamp":1720710516525,"user_tz":180,"elapsed":242,"user":{"displayName":"Alfred Kofi ADZIKA","userId":"16750235551968629195"}},"outputId":"075b3ab2-bc58-49c5-a178-c41a1af7767e"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'data_dist_keys' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-c95cdcb20f50>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mbeta_dict_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mbeta_dict_errors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata_dist_key\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_dist_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;31m# distribution_errors = []\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m#distribution_loss_repeat = {}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'data_dist_keys' is not defined"]}],"source":["# beta vrs losses vrs errors\n","k =50\n","d = 2 # number of features\n","#alpha_ = 0.2 # Learning rate associated with the means\n","#beta_ = 0.01 # Balances the size of the clusters\n","n_train = 10000\n","n_test = 1000\n","n_repeats = 1\n","alpha = 0.2\n","# beta = 0.01\n","n_ind = 1000\n","\n","betas = np.linspace(-0.5,1.5, 15)\n","beta_dict_loss = {}\n","beta_dict_errors = {}\n","\n","for beta in betas:\n","        a = 1\n","        loss_vals = {}\n","        cost = {}\n","        error = {}\n","        beta_dict_loss[beta] = []\n","        beta_dict_errors[beta] = []\n","        for data_dist_key in data_dist_keys:\n","            # distribution_errors = []\n","            #distribution_loss_repeat = {}\n","            error[data_dist_key] = []\n","\n","            data = generate_data_methods[data_dist_key](n_train + n_test)\n","            training_data = data[:n_train]\n","            test_data = data[n_train:]\n","            assert len(training_data) == n_train\n","            assert len(test_data) == n_test\n","            #training_data = generate_data(n_train)\n","            #test_data = generate_data(n_test)\n","            kmeans = OnlineBalancedKmeans( k, d, alpha = alpha, beta = beta, inference_method={'method': 'weights', 'alpha': 0.4, 'beta':7 } , data_distribution=data_dist_key) # TODO. Copy might not be necessary.\n","            # Computing the losses and error and storing them in a dict\n","            inference_functions = [\n","                kmeans.infer_euclid_dist,\n","                kmeans.infer_norm_weights,\n","                kmeans.infer_cluster_size,\n","                kmeans.infer_weights,\n","                kmeans.infer_merge_norm_cl_size,\n","                kmeans.infer_merge_norm_ecl_dis,\n","                kmeans.infer_cluster_size_exp\n","            ]\n","            # Initializing losses\n","            cost[data_dist_key] = [ kmeans_loss(kmeans, training_data) ]\n","            # Initializing errors\n","            total_error = np.zeros_like(inference_functions)\n","            for datum in test_data:\n","                true_value = datum[-1]\n","                predicted_values = [infer(datum) for infer in inference_functions ] #dim = (n_inf,) #kmeans.infer(datum)\n","                total_error += np.array([compute_error(true_value, pred_value) for pred_value in predicted_values])\n","                # total_error dim = (n_inf)\n","            error[data_dist_key].append(total_error.tolist())\n","            for index, datum in enumerate(training_data):\n","                kmeans.assign_and_update(datum)\n","                if index%n_ind == 0:\n","                    # Compute losses after each 1000 assignments\n","                    cost[data_dist_key].append( kmeans_loss(kmeans, training_data) )\n","                    # Compute errors after each 1000 assignments\n","                        # Predicted values for all inference methods\n","                    predicted_values = [infer(datum) for infer in inference_functions ]\n","                    total_error = np.zeros_like(inference_functions)\n","                    for datum in test_data:\n","                        true_value = datum[-1]\n","                        predicted_values = [infer(datum) for infer in inference_functions ] #dim = (n_inf,) #kmeans.infer(datum)\n","                        total_error += np.array([compute_error(true_value, pred_value) for pred_value in predicted_values])\n","                        # total_error dim = (n_inf)\n","                    error[data_dist_key].append( total_error.tolist() )\n","        beta_dict_errors[beta] = list(error.values())\n","        #mean_losses = np.mean(list(cost.values()), 0) # Values recorded for every 100 iteration\n","        beta_dict_loss[beta] = list(cost.values())\n","\n","#         mean_losses = np.mean(list(cost.values()), 0) # Values recorded for every 100 iteration\n","#         beta_dict_loss[beta].append(mean_losses)\n","#         mean_errors = np.mean(list(error.values()), 0) # Values recorded for every 100 iteration\n","#         beta_dict_errors[beta].append(mean_errors)"]},{"cell_type":"markdown","id":"honey-gather","metadata":{"id":"honey-gather"},"source":["## Working on the data"]},{"cell_type":"code","execution_count":null,"id":"chubby-district","metadata":{"id":"chubby-district"},"outputs":[],"source":["# Extracting the data into an array\n","list_ae = np.array(list(alpha_dict_errors.values()))\n","list_al = np.array(list(alpha_dict_loss.values()))\n","list_be = np.array(list(beta_dict_errors.values()))\n","list_bl = np.array(list(beta_dict_loss.values()))\n","list_ke = np.array(list(kdict_errors.values()))\n","list_kl = np.array(list(kdict_loss.values()))\n","lists = [list_ae, list_al, list_be, list_bl, list_ke, list_kl]"]},{"cell_type":"code","execution_count":null,"id":"fifty-search","metadata":{"id":"fifty-search"},"outputs":[],"source":["# alpha\n","infer_err_a = list_ae.mean(axis = 0).mean(axis = 0).mean(axis = 0).T # Avg over the alphas and the data\n","infer_err_a_b = list_ae.mean(axis = 1).mean(axis = 1)[6].T/1000 # best alpha is at index 6, alpha = 0.6"]},{"cell_type":"code","execution_count":null,"id":"portable-framing","metadata":{"id":"portable-framing"},"outputs":[],"source":["# beta\n","infer_err_b = list_be.mean(axis = 0).mean(axis = 0).T # Avg over the betas and the data\n","infer_err_b_b = list_be.mean(axis = 1)[4].T # best alpha is at index 6, alpha = 0.6"]},{"cell_type":"code","execution_count":null,"id":"hourly-nepal","metadata":{"id":"hourly-nepal"},"outputs":[],"source":["# ks\n","infer_err_k = list_be.mean(axis = 0).mean(axis = 0).T # Avg over the betas and the data\n","infer_err_k_b = list_be.mean(axis = 1)[2].T # best alpha is at index 6, alpha = 0.6"]},{"cell_type":"code","execution_count":null,"id":"quick-egypt","metadata":{"id":"quick-egypt"},"outputs":[],"source":["## Organising the parameters\n","dla_er = np.array(list(dict_la_er.values()))\n","dlb_er = np.array(list(dict_lb_er.values()))\n","# dlk_er = np.array(list(dict_lk_er.values()))\n","dict_hyp_ers = [dla_er, dlb_er, dlk_er]\n","\n","alphas = np.linspace(0,1, 11 )\n","betas = np.linspace(-0.5,1.5, 15)\n","ks = np.array(list(range(10,1001,100)))\n","x = np.array(list(range(0, 10001, 1000)))\n","list_hyps = [alphas, betas, kss]\n","\n","zip_h_d = zip(list_hyps, dict_hyp_ers)"]},{"cell_type":"markdown","id":"average-daniel","metadata":{"id":"average-daniel"},"source":["### Sketching the graphs of alphas and their corresponding performance"]},{"cell_type":"code","execution_count":null,"id":"significant-broadcasting","metadata":{"id":"significant-broadcasting"},"outputs":[],"source":["## ## Sketching alphas and their corresponding performance\n","fig, (ax1,ax2) = plt.subplots(1, 2, figsize= (12, 5))\n","\n","hyp_names = [r'$\\alpha$',r'$\\beta$','number of cluster']\n","\n","\n","for hyps, dict_hyp_er, hyp_name  in zip([list_hyps[0]], [dict_hyp_ers[0]],[hyp_names[0]]):\n","    labels = []\n","    for hyp_index, hyp in enumerate(hyps):\n","\n","        if dict_hyp_er[inf_index,hyp_index,:][-1] < 10**8:\n","            y = dict_hyp_er[0,hyp_index,:].tolist()\n","            labels.append(round(hyp,2))\n","            #ci = 1.96 * np.std(y)/np.sqrt(len(x))\n","\n","            ax2.plot(x, y)\n","            #ax2.fill_between(x, (y-ci), (y+ci), alpha=0.2)\n","            ax2.legend(labels,loc = 4, framealpha = 0.2)\n","        #plt.show()\n","        #print('{}'.format(dict_hyp_er[inf_index,hyp_index,:].shape))\n","        ax2.set_xlabel('Number of assignments')\n","        ax2.set_ylabel('Inference errors')\n","        ax2.set_title(\"Errors of the inference methods over {}'s\".format(hyp_name))\n","        # fig.savefig('{} err.png'.format(hyp_name))\n","\n","for hyps, dict_hyp_loss, hyp_name  in zip([list_hyps[0]], [dict_hyp_losses[0]],[hyp_names[0]]):\n","    labels = []\n","    for hyp_index, hyp in enumerate(hyps):\n","        if dict_hyp_loss[hyp_index,:][6] >= 100:\n","            labels.append(round(hyp,2))\n","            y = dict_hyp_loss[hyp_index,:]\n","            #ci = 1.96 * np.std(y)/np.sqrt(len(x))\n","\n","            ax1.plot(x, y)\n","            #ax1.fill_between(x, (y-ci), (y+ci), alpha=0.1)\n","            ax1.legend(labels,loc = 1, framealpha = 0.2)\n","    #plt.show()\n","    #print('{}'.format(dict_hyp_er[inf_index,hyp_index,:].shape))\n","    ax1.set_xlabel('Number of assignments')\n","    ax1.set_ylabel('Losses')\n","    ax1.set_title(\"Errors of the inference methods over {}'s\".format(hyp_name))\n","plt.savefig('{} loss_err.png'.format(hyp_name))"]},{"cell_type":"markdown","id":"monetary-density","metadata":{"id":"monetary-density"},"source":["### Sketching the graphs of number of clusters and their corresponding performance"]},{"cell_type":"code","execution_count":null,"id":"behavioral-girlfriend","metadata":{"id":"behavioral-girlfriend"},"outputs":[],"source":["## Sketching number of clusters and their corresponding performance\n","fig, (ax1,ax2) = plt.subplots(1, 2, figsize= (12, 5))\n","\n","hyp_names = ['alpha','beta','number of cluster']\n","for hyps, dict_hyp_loss, hyp_name  in zip([list_hyps[2]], [dict_hyp_losses[2]],[hyp_names[2]]):\n","    labels = []\n","    for hyp_index, hyp in enumerate(hyps):\n","        if dlk_ls[hyp_index,0] < 1*10**3:\n","            labels.append(round(hyp,2))\n","            y = dlk_ls[hyp_index,:]\n","            # ci = 1.96 * np.std(y)/np.sqrt(len(x))\n","\n","            ax1.plot(x, y)\n","            # ax1.fill_between(x, (y-ci), (y+ci), alpha=0.1)\n","            ax1.legend(labels,loc = 1, framealpha = 0.2)\n","    #plt.show()\n","    #print('{}'.format(dict_hyp_er[inf_index,hyp_index,:].shape))\n","    ax1.set_xlabel('Number of assignments')\n","    ax1.set_ylabel('Losses')\n","    ax1.set_title('Losses of the inference methods over {}s'.format(hyp_name))\n","# plt.savefig('{} loss.png'.format(hyp_name))\n","\n","\n","for hyps, dict_hyp_er, hyp_name  in zip([list_hyps[2]], [dict_hyp_ers[2]],[hyp_names[2]]):\n","    labels = []\n","    for hyp_index, hyp in enumerate(hyps):\n","        if dlk_er[hyp_index,-1] < 3*10**2:\n","            labels.append(round(hyp,2))\n","            y = dlk_er[hyp_index, :].tolist()\n","            # ci = 1.96 * np.std(y)/np.sqrt(len(x))\n","\n","            ax2.plot(x, y)\n","            # ax2.fill_between(x, (y-ci), (y+ci), alpha=0.1)\n","            ax2.legend(labels,loc = 2, framealpha = 0.2)\n","        #plt.show()\n","        #print('{}'.format(dict_hyp_er[inf_index,hyp_index,:].shape))\n","        ax2.set_xlabel('Number of assignments')\n","        ax2.set_ylabel('Inference errors')\n","        ax2.set_title('Errors of the inference methods over {}s'.format(hyp_name))\n","        #plt.savefig('{} err.png'.format(hyp_name))\n","plt.savefig('{} loss_err.png'.format(hyp_name))"]},{"cell_type":"markdown","id":"rough-induction","metadata":{"id":"rough-induction"},"source":["### Sketching the graphs of Betas and their corresponding performance\n"]},{"cell_type":"code","execution_count":null,"id":"differential-charm","metadata":{"id":"differential-charm"},"outputs":[],"source":["## Sketching Beta's Errors and Losses\n","fig, (ax1,ax2) = plt.subplots(1, 2, figsize= (12, 5))\n","\n","hyp_names = [r'$\\alpha$',r'$\\beta$','number of cluster']\n","for hyps, dict_hyp_loss, hyp_name  in zip([list_hyps[1]], [dict_hyp_losses[1]],[hyp_names[1]]):\n","    labels = []\n","    for hyp_index, hyp in enumerate(hyps):\n","        if dict_hyp_loss[hyp_index,:][6] < 600:\n","            labels.append(round(hyp,2))\n","            y = dict_hyp_loss[hyp_index,:]\n","            #ci = 1.96 * np.std(y)/np.sqrt(len(x))\n","\n","            ax1.plot(x, y)\n","            #ax1.fill_between(x, (y-ci), (y+ci), alpha=0.1)\n","            ax1.legend(labels,loc = 1, framealpha = 0.2)\n","    #plt.show()\n","    #print('{}'.format(dict_hyp_er[inf_index,hyp_index,:].shape))\n","    ax1.set_xlabel('Number of assignments')\n","    ax1.set_ylabel('Losses')\n","    ax1.set_title('Losses of the inference methods over {}s'.format(hyp_name))\n","# plt.savefig('{} loss.png'.format(hyp_name))\n","\n","for hyps, dict_hyp_er, hyp_name  in zip([list_hyps[1]], [dict_hyp_ers[1]], [hyp_names[1]]):\n","    labels = []\n","    for hyp_index, hyp in enumerate(hyps):\n","\n","        if dict_hyp_er[inf_index,hyp_index,:][-1] < 6*10**7:\n","            labels.append(round(hyp,2))\n","            y = dict_hyp_er[0,hyp_index,:].tolist()\n","            #ci = 1.96 * np.std(y)/np.sqrt(len(x))\n","\n","            ax2.plot(x, y)\n","            #ax2.fill_between(x, (y-ci), (y+ci), alpha=0.1)\n","            ax2.legend(labels,loc = 2, framealpha = 0.1)\n","        #plt.show()\n","        #print('{}'.format(dict_hyp_er[inf_index,hyp_index,:].shape))\n","        ax2.set_xlabel('Number of assignments')\n","        ax2.set_ylabel('Inference errors')\n","        ax2.set_title('Errors of the inference methods over {}s'.format(hyp_name))\n","        #ax2.set_savefig('{} err.png'.format(hyp_name))\n","plt.savefig('{} loss_err.png'.format(hyp_name))"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.2"}},"nbformat":4,"nbformat_minor":5}